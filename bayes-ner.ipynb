{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "from itertools import groupby\n",
    "\n",
    "ans_path = './ner-mturk/answers*'\n",
    "ans_file_list = sorted(glob.glob(ans_path))\n",
    "single_ans_list = sorted(glob.glob('ner-mturk/single_ans*'))\n",
    "truth_file = './ner-mturk/ground_truth.txt'\n",
    "path_w = './MBEM_outputs'\n",
    "alltask_num = 5985\n",
    "train_num = 5385\n",
    "train_num5 = 1077\n",
    "test_num = 600\n",
    "redundancy = 5\n",
    "iteration_times = 5\n",
    "ans_file = ans_file_list[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_output(path, class_dic_r, to_label=False):\n",
    "    f = open(path)\n",
    "    make_xypair = lambda v: list(zip(*[l.strip().split() for l in v]))\n",
    "    is_emptyline = lambda x: x.strip() != '' and not x.startswith('-DOCSTART-')\n",
    "    list_int = lambda t: list([class_dic_r[int(v)] for v in t])\n",
    "    xypairs = [make_xypair(v) for k, v in groupby(f, is_emptyline) if k]\n",
    "    if to_label == True:\n",
    "        return ([p[0] for p in xypairs], [list(list_int(p[1])) for p in xypairs])\n",
    "    if len(xypairs[0]) == 2:\n",
    "        return ([p[0] for p in xypairs], [list(p[1]) for p in xypairs])\n",
    "    else:\n",
    "        return ([p[0] for p in xypairs], [list(p[1:]) for p in xypairs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_crowd_data(ans_file, truth_file, train_num):\n",
    "    #answers_list[task_num] = [[words, ...], [worker], [[answers]...]]\n",
    "    answers_list = []\n",
    "    sentence = []\n",
    "    answers = []\n",
    "    worker = []\n",
    "    words_dic = {}\n",
    "    get_worker = lambda x: tuple([i for i, l in enumerate(x) if l !='?'])\n",
    "    for l in open(ans_file):\n",
    "        ans = l.split()\n",
    "        if len(l) < 2:\n",
    "            answers_list.append([sentence, worker, answers])\n",
    "            sentence = []\n",
    "            worker = []\n",
    "            answers = []\n",
    "        else:\n",
    "            if len(worker) < 1:\n",
    "                worker = get_worker(ans[1:])\n",
    "            if not ans[0] in words_dic:\n",
    "                words_dic[ans[0]] = len(words_dic)\n",
    "            sentence.append(ans[0])\n",
    "            answers.append(get_ans(worker, ans[1:]))            \n",
    "    #truth_list: [(task_word, answer), ....]\n",
    "    truth_list = [tuple(l.split()) for l in open(truth_file) if l != \"\\n\"]\n",
    "    ans_train = answers_list[0:train_num]\n",
    "#    ans_test = answers_list[train_num:]\n",
    "    return ans_train, words_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ans(workers, answers):\n",
    "    ans_label = []\n",
    "    for a in workers:\n",
    "        ans_label.append(class_dic[answers[a]])\n",
    "    return ans_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worker_num 46\n",
      "クラス： {'B-LOC': 0, 'B-MISC': 1, 'B-ORG': 2, 'B-PER': 3, 'I-LOC': 4, 'I-MISC': 5, 'I-ORG': 6, 'I-PER': 7, 'O': 8}\n",
      "クラス数： 9\n",
      "タスク数 1077\n",
      "タスク数(単語): 14119\n",
      "語彙数 6450\n"
     ]
    }
   ],
   "source": [
    "class_list = list(set(l.split()[1] for l in open(truth_file) if l != \"\\n\"))\n",
    "class_dic = {k:i for i, k in enumerate(sorted(class_list))}\n",
    "class_dic_r = {class_dic[c]:c for c in class_dic}\n",
    "class_label = {'LOC': 0, 'MISC': 1, 'ORG': 2, 'PER': 3, 'O': 4}\n",
    "labelnum_dic = {'B-LOC': 0, 'B-MISC': 1, 'B-ORG': 2, 'B-PER': 3, 'I-LOC': 0, 'I-MISC': 1, 'I-ORG': 2, 'I-PER': 3, 'O': 4}\n",
    "#データの読み込み\n",
    "ans_train, words_dic = load_crowd_data(ans_file, truth_file, train_num5)\n",
    "X_true, y_true = read_output('../MBEM/ner-mturk/ground_truth.txt', class_dic_r)\n",
    "X_true5, y_true5 = read_output('../MBEM/ner-mturk/testset5.txt', class_dic_r)\n",
    "workers_count = []\n",
    "for i in range(len(ans_train)):\n",
    "    for w in ans_train[i][1]:\n",
    "        workers_count.append(w)\n",
    "worker_num = len(set(workers_count))\n",
    "class_num = len(class_dic)\n",
    "#多分データを変えるごとにクラス確認したくなるので残しておく\n",
    "num_words = 0\n",
    "for l in ans_train:\n",
    "    num_words += len(l[0])\n",
    "print(\"worker_num\", worker_num)\n",
    "print(\"クラス：\",class_dic)\n",
    "print(\"クラス数：\", class_num)\n",
    "print(\"タスク数\",len(ans_train))\n",
    "print(\"タスク数(単語):\", num_words)\n",
    "print(\"語彙数\", len(words_dic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input \n",
    "#ラベル :c\n",
    "#単語: x\n",
    "\n",
    "K = worker_num\n",
    "J = class_num\n",
    "#初期値を設定する\n",
    "conf = np.zeros((K,J,J,J)) #worker,true_label,woker_ans(n-1),worker_ans(n)\n",
    "rho = np.zeros((J)) #出力確率\n",
    "trans_prob = np.zeros((J,J))\n",
    "\n",
    "#前向き後ろ向きアルゴリズム\n",
    "#論文の7式\n",
    "\n",
    "#ビタビアルゴリズム\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_symbol = [ \"w1\", \"w2\", \"w3\" ] # 状態\n",
    "c = len(s_symbol)               # 状態数\n",
    "v_symbol = [ \"v1\", \"v2\" ]       # 出力記号\n",
    "m = len(v_symbol)               # 出力数\n",
    "\n",
    "# 遷移確率行列\n",
    "A = np.array([[0.1, 0.7, 0.2], [0.2, 0.1, 0.7], [0.7, 0.2, 0.1]])\n",
    "# 出力確率行列\n",
    "B = np.array([[0.9, 0.1], [0.6, 0.4], [0.1, 0.9]])\n",
    "# 初期確率行列\n",
    "p = np.array([1/3, 1/3, 1/3])\n",
    "\n",
    "class HMM:\n",
    "\n",
    "    def __init__(self, p, A, B): \n",
    "        self.p = p\n",
    "        self.A = A\n",
    "        self.B = B\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        n = len(x) # 観測回数\n",
    "\n",
    "        # 前向きアルゴリズム\n",
    "        alpha = np.zeros((n, c))\n",
    "\n",
    "        # Step1\n",
    "        alpha[0, :] = p[:] * B[:, x[0]]\n",
    "\n",
    "        # Step2\n",
    "        for t in range(1, n):\n",
    "            alpha[t, :] = np.dot(alpha[t-1, :], A) * B[:, x[t]]\n",
    "\n",
    "        # Step3\n",
    "        return round(np.sum(alpha[-1]), 3)\n",
    "\n",
    "    def backward(self, x, c):\n",
    "        n = len(x) # 観測回数\n",
    "\n",
    "        # 後ろ向きアルゴリズム\n",
    "        # Step1\n",
    "        beta = np.zeros((n, c))\n",
    "        beta[-1, :] = 1\n",
    "\n",
    "        # Step2\n",
    "        for t in range((n-1), 0, -1):\n",
    "            beta[t-1, :] = np.dot(self.A, (self.B[:, x[t]] * beta[t, :]))\n",
    "\n",
    "        # Step3\n",
    "        return round(sum(p[:] * self.B[:, x[0]] * beta[0, :]), 3)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    # 観測結果\n",
    "    x = [ 0, 1, 0 ] # v1, v2, v1という結果\n",
    "\n",
    "    hmm = HMM(p, A, B)\n",
    "    print(hmm.forward(x, c))\n",
    "    print(hmm.backward(x, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_prob_DS(est_labels, ans_train, k, worker_num, num_words):\n",
    "    #学習機とラベルの更新\n",
    "    n = len(ans_train)\n",
    "    m = worker_num\n",
    "    e_conf = np.zeros((m,k,k,k))\n",
    "    temp_conf = (1/float(k))*np.ones((m,k,k,k))\n",
    "    labels_md = 0.000001 * np.ones((k,k)) #正規化の時に困りそうなので0にしないでおく\n",
    "    task_acc = np.zeros((n, k, k))\n",
    "    #est_labelsを正解ラベルとして混同行列を作成 式(7)\n",
    "    for i in range(n):\n",
    "        pre_ans = ans_train[i][2][0]\n",
    "        for l in range(len(ans_train[i][2])-1):\n",
    "            l += 1\n",
    "            #真のラベルの周辺分布を更新\n",
    "            labels_md[int(est_labels[i][l-1]),int(est_labels[i][l])] += 1\n",
    "            for a, j in enumerate(ans_train[i][1]): #各回答ワーカーについて\n",
    "                temp_conf[j,int(est_labels[i][l]),:,:] = temp_conf[j,int(est_labels[i][l]),:,:] + np.outer(np.eye(k)[ans_train[i][2][l-1][a]],np.eye(k)[ans_train[i][2][l][a]]) #外積 混同行列の作成\n",
    "    #temp_confの正規化\n",
    "    for j in range(m):  \n",
    "        for p in range(k):\n",
    "            e_conf[j,p,:,:] = np.divide(temp_conf[j,p,:,:],np.outer(np.sum(temp_conf[j,p,:,:],axis =1),np.ones(k)))\n",
    "    labels_md = np.divide(labels_md, np.sum(labels_md)*np.ones(k))\n",
    "    #混同行列を使って真のラベル推定(論文p5の(5)式)\n",
    "    for i in range(n):\n",
    "        for l in range(len(ans_train[i][2])):\n",
    "            temp_class = 0.0\n",
    "            for r, a in enumerate(ans_train[i][1]):\n",
    "                if l == 0:\n",
    "                    w_conf = np.sum(e_conf[a,:,:,:], axis=1)\n",
    "                else:\n",
    "                    w_conf = e_conf[a,:,ans_train[i][2][l-1][r],:]\n",
    "                #混同行列の該当列をとる\n",
    "                temp_acc = np.log(np.dot(w_conf,np.transpose(np.eye(k)[ans_train[i][2][l][r]])))\n",
    "                temp_class = temp_class + temp_acc\n",
    "            temp_class = np.log(labels_md) +temp_class\n",
    "            est_labels[i][l] = np.argmax(temp_class)%9\n",
    "    return est_labels, e_conf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
